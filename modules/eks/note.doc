To create a eks cluster we need 2 IAM roles:
1. EKS Cluster Role: This role is used by the EKS service to manage the cluster.
   It typically includes permissions for creating
   and managing resources like EC2 instances, VPCs, and security groups.


2. EKS Node Group Role: This role is used by the worker nodes in the EKS cluster. 
   It allows the nodes to interact with AWS services, such as pulling container images from ECR,
   writing logs to CloudWatch, and accessing other AWS resources.


IAM roles need to be created with the necessary policies attached.
The policies should include permissions for EKS operations, EC2 instance management,
VPC management, and any other AWS services that the EKS cluster will interact with.

Two separate polices need for the two roles:

1. EKS Cluster Role Policy: This policy should include permissions for EKS operations, 
   such as creating and managing clusters, managing VPCs, and handling security groups.
2. EKS Node Group Role Policy: This policy should include permissions for EC2 instance management, 
   ECR access, CloudWatch logging, and any other necessary AWS services that the worker nodes will
   need to interact with.   

aws_iam_role_policy_attachment

The aws_iam_role_policy_attachment resource in Terraform attaches an IAM policy to an IAM role.

Explanation:

It links a specific policy (like AmazonEKSClusterPolicy) to a role (like your EKS cluster role).
This grants the role the permissions defined in the policy.
For example, attaching AmazonEKSClusterPolicy to the cluster role allows the EKS control plane to manage AWS resources needed for the cluster.

In summary:
This resource ensures your IAM roles have the necessary permissions by associating them with AWS-managed or custom policies.

AWS EKS Cluster

The aws_eks_cluster resource in Terraform creates an Amazon EKS (Elastic Kubernetes Service) cluster.

Key points:

name: The name of your EKS cluster.
version: The Kubernetes version for the cluster.
role_arn: The IAM role that EKS uses to manage AWS resources (must have the correct trust and permissions).
vpc_config: Specifies networking details, including the subnets where the cluster will run.
depends_on: Ensures the IAM role and policy attachment are created before the cluster.
Purpose: This resource provisions the EKS control plane, which manages Kubernetes workloads and worker nodes in your specified VPC and subnets. 
It is the core resource for running Kubernetes on AWS.


The aws_eks_node_group resource in Terraform creates a managed group of EC2 worker nodes for your EKS cluster.

Key points:

cluster_name: The name of the EKS cluster these nodes will join.
node_group_name: The name for this node group.
node_role_arn: The IAM role that grants permissions for the nodes to interact with AWS and EKS.
subnet_ids: The subnets where the nodes will be launched.
instance_types: The EC2 instance types for the nodes.
capacity_type: Specifies if the nodes are On-Demand or Spot instances.
scaling_config: Sets the desired, minimum, and maximum number of nodes in the group.

Purpose: This resource provisions and manages the lifecycle of worker nodes for your Kubernetes workloads, 
handling scaling, updates, and integration with the EKS control plane.

In AWS EKS (Elastic Kubernetes Service):

Private subnets are used for worker nodes (EC2 instances) and internal resources. They are not directly accessible from the internet, which improves security. EKS recommends placing your cluster nodes in private subnets.
Public subnets are typically used for resources that need direct internet access, such as load balancers (e.g., AWS ELB/NLB) or NAT gateways. These subnets have a route to an internet gateway.
In summary:

Worker nodes and internal services → private subnets
Load balancers and NAT gateways → public subnets

The aws_eks_cluster resource is created inside a VPC, but it does not directly reside "inside" a subnet. 
Instead, it is associated with one or more subnets for networking purposes.

The EKS control plane itself is a managed AWS service and is deployed within your specified VPC.
You specify which subnets (usually private subnets) the cluster uses for communication with worker nodes and other resources.
The cluster does not have an EC2 instance or endpoint inside a subnet; it uses the subnets for networking and integration.

In summary:

EKS cluster is associated with a VPC and its subnets, but is not an EC2 instance inside a subnet.
Worker nodes (EC2 instances) are launched inside the specified subnets.

